-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Work stealing scheduler.
--   
--   A work stealing scheduler that is designed for parallelization of
--   heavy work loads. It was primarily developed for <a>massiv</a> array
--   library, but it is general enough to be useful for any computation
--   that fits the model of few workers and many jobs.
@package scheduler
@version 2.0.0.1


module Control.Scheduler

-- | Main type for scheduling work. See <a>withScheduler</a> or
--   <a>withScheduler_</a> for ways to construct and use this data type.
data Scheduler s a

-- | This is a wrapper around <a>Scheduler</a>, but it also keeps a
--   separate state for each individual worker. See <a>withSchedulerWS</a>
--   or <a>withSchedulerWS_</a> for ways to construct and use this data
--   type.
data SchedulerWS ws a

-- | Computed results of scheduled jobs.
data Results a

-- | Finished normally with all scheduled jobs completed
Finished :: [a] -> Results a

-- | Finished early by the means of <a>cancelBatch</a> or <a>terminate</a>.
FinishedEarly :: [a] -> !a -> Results a

-- | Finished early by the means of <a>cancelBatchWith</a> or
--   <a>terminateWith</a>.
FinishedEarlyWith :: !a -> Results a

-- | Initialize a scheduler and submit jobs that will be computed
--   sequentially or in parallelel, which is determined by the
--   <a>Comp</a>utation strategy.
--   
--   Here are some cool properties about the <a>withScheduler</a>:
--   
--   <ul>
--   <li>This function will block until all of the submitted jobs have
--   finished or at least one of them resulted in an exception, which will
--   be re-thrown at the callsite.</li>
--   <li>It is totally fine for nested jobs to submit more jobs for the
--   same or other scheduler</li>
--   <li>It is ok to initialize multiple schedulers at the same time,
--   although that will likely result in suboptimal performance, unless
--   workers are pinned to different capabilities.</li>
--   <li><b>Warning</b> It is pretty dangerous to schedule jobs that can
--   block, because it might lead to a potential deadlock, if you are not
--   careful. Consider this example. First execution works fine, since
--   there are two scheduled workers, and one can unblock the other, but
--   the second scenario immediately results in a deadlock.</li>
--   </ul>
--   
--   <pre>
--   &gt;&gt;&gt; withScheduler (ParOn [1,2]) $ \s -&gt; newEmptyMVar &gt;&gt;= (\ mv -&gt; scheduleWork s (readMVar mv) &gt;&gt; scheduleWork s (putMVar mv ()))
--   [(),()]
--   
--   &gt;&gt;&gt; import System.Timeout
--   
--   &gt;&gt;&gt; timeout 1000000 $ withScheduler (ParOn [1]) $ \s -&gt; newEmptyMVar &gt;&gt;= (\ mv -&gt; scheduleWork s (readMVar mv) &gt;&gt; scheduleWork s (putMVar mv ()))
--   Nothing
--   </pre>
--   
--   <b>Important</b>: In order to get work done truly in parallel, program
--   needs to be compiled with <tt>-threaded</tt> GHC flag and executed
--   with <tt>+RTS -N -RTS</tt> to use all available cores.
withScheduler :: MonadUnliftIO m => Comp -> (Scheduler RealWorld a -> m b) -> m [a]

-- | Same as <a>withScheduler</a>, but discards results of submitted jobs.
withScheduler_ :: MonadUnliftIO m => Comp -> (Scheduler RealWorld a -> m b) -> m ()

-- | Same as <a>withScheduler</a>, except instead of a list it produces
--   <a>Results</a>, which allows for distinguishing between the ways
--   computation was terminated.
withSchedulerR :: MonadUnliftIO m => Comp -> (Scheduler RealWorld a -> m b) -> m (Results a)

-- | Run a scheduler with stateful workers. Throws <a>MutexException</a> if
--   an attempt is made to concurrently use the same <a>WorkerStates</a>
--   with another <a>SchedulerWS</a>.
--   
--   <h4><b>Examples</b></h4>
--   
--   A good example of using stateful workers would be generation of random
--   number in parallel. A lof of times random number generators are not
--   thread safe, so we can work around this problem with a separate
--   stateful generator for each of the workers.
--   
--   <pre>
--   &gt;&gt;&gt; import Control.Monad as M ((&gt;=&gt;), replicateM)
--   
--   &gt;&gt;&gt; import Control.Concurrent (yield, threadDelay)
--   
--   &gt;&gt;&gt; import Data.List (sort)
--   
--   &gt;&gt;&gt; -- ^ Above imports are used to make sure output is deterministic, which is needed for doctest
--   
--   &gt;&gt;&gt; import System.Random.MWC as MWC
--   
--   &gt;&gt;&gt; import Data.Vector.Unboxed as V (singleton)
--   
--   &gt;&gt;&gt; states &lt;- initWorkerStates (ParN 4) (MWC.initialize . V.singleton . fromIntegral . getWorkerId)
--   
--   &gt;&gt;&gt; let scheduleGen scheduler = scheduleWorkState scheduler (MWC.uniform &gt;=&gt; \r -&gt; yield &gt;&gt; threadDelay 200000 &gt;&gt; pure r)
--   
--   &gt;&gt;&gt; sort &lt;$&gt; withSchedulerWS states (M.replicateM 4 . scheduleGen) :: IO [Double]
--   [0.21734983682025255,0.5000843862105709,0.5759825622603018,0.8587171114177893]
--   
--   &gt;&gt;&gt; sort &lt;$&gt; withSchedulerWS states (M.replicateM 4 . scheduleGen) :: IO [Double]
--   [2.3598617298033475e-2,9.949679290089553e-2,0.38223134248645885,0.7408640677124702]
--   </pre>
--   
--   In the above example we use four different random number generators
--   from <a>`mwc-random`</a> in order to generate 4 numbers, all in
--   separate threads. The subsequent call to the <a>withSchedulerWS</a>
--   function with the same <tt>states</tt> is allowed to reuse the same
--   generators, thus avoiding expensive initialization.
--   
--   <i>Side note</i> - The example presented was crafted with slight
--   trickery in order to guarantee that the output is deterministic, so if
--   you run instructions exactly the same way in GHCI you will get the
--   exact same output. Non-determinism comes from thread scheduling,
--   rather than from random number generator, because we use exactly the
--   same seed for each worker, but workers run concurrently. Exact output
--   is not really needed, except for the doctests to pass.
withSchedulerWS :: MonadUnliftIO m => WorkerStates ws -> (SchedulerWS ws a -> m b) -> m [a]

-- | Run a scheduler with stateful workers, while discarding computation
--   results.
withSchedulerWS_ :: MonadUnliftIO m => WorkerStates ws -> (SchedulerWS ws () -> m b) -> m ()

-- | Same as <a>withSchedulerWS</a>, except instead of a list it produces
--   <a>Results</a>, which allows for distinguishing between the ways
--   computation was terminated.
withSchedulerWSR :: MonadUnliftIO m => WorkerStates ws -> (SchedulerWS ws a -> m b) -> m (Results a)

-- | Get the underlying <a>Scheduler</a>, which cannot access
--   <a>WorkerStates</a>.
unwrapSchedulerWS :: SchedulerWS ws a -> Scheduler RealWorld a

-- | The most basic scheduler that simply runs the task instead of
--   scheduling it. Early termination requests are bluntly ignored.
trivialScheduler_ :: Scheduler s ()

-- | This trivial scheduler will behave in the same way as
--   <a>withScheduler</a> with <a>Seq</a> computation strategy, except it
--   is restricted to <a>PrimMonad</a>, instead of <a>MonadUnliftIO</a>.
withTrivialScheduler :: MonadPrim s m => (Scheduler s a -> m b) -> m [a]

-- | This trivial scheduler will behave in a similar way as
--   <a>withSchedulerR</a> with <a>Seq</a> computation strategy, except it
--   is restricted to <a>PrimMonad</a>, instead of <a>MonadUnliftIO</a> and
--   the work isn't scheduled, but rather computed immediately.
withTrivialSchedulerR :: forall a b m s. MonadPrim s m => (Scheduler s a -> m b) -> m (Results a)

-- | Schedule an action to be picked up and computed by a worker from a
--   pool of jobs. Similar to <a>scheduleWorkId</a>, except the job doesn't
--   get the worker id.
scheduleWork :: MonadPrimBase s m => Scheduler s a -> m a -> m ()

-- | Same as <a>scheduleWork</a>, but only for a <a>Scheduler</a> that
--   doesn't keep the results.
scheduleWork_ :: MonadPrimBase s m => Scheduler s () -> m () -> m ()

-- | Schedule an action to be picked up and computed by a worker from a
--   pool of jobs. Argument supplied to the job will be the id of the
--   worker doing the job. This is useful for identification of a thread
--   that will be doing the work, since there is one-to-one mapping from
--   <a>ThreadId</a> to <a>WorkerId</a> for a particular scheduler.
scheduleWorkId :: MonadPrimBase s m => Scheduler s a -> (WorkerId -> m a) -> m ()

-- | Same as <a>scheduleWorkId</a>, but only for a <a>Scheduler</a> that
--   doesn't keep the results.
scheduleWorkId_ :: MonadPrimBase s m => Scheduler s () -> (WorkerId -> m ()) -> m ()

-- | Schedule a job that will get a worker state passed as an argument
scheduleWorkState :: MonadPrimBase RealWorld m => SchedulerWS ws a -> (ws -> m a) -> m ()

-- | Same as <a>scheduleWorkState</a>, but dont' keep the result of
--   computation.
scheduleWorkState_ :: MonadPrimBase RealWorld m => SchedulerWS ws () -> (ws -> m ()) -> m ()

-- | Schedule the same action to run <tt>n</tt> times concurrently. This
--   differs from <a>replicateConcurrently</a> by allowing the caller to
--   use the <a>Scheduler</a> freely, or to allow early termination via
--   <a>terminate</a> across all (identical) threads. To be called within a
--   <a>withScheduler</a> block.
replicateWork :: MonadPrimBase s m => Scheduler s a -> Int -> m a -> m ()

-- | Same as <a>replicateWork</a>, but it does not retain the results of
--   scheduled jobs
replicateWork_ :: MonadPrimBase s m => Scheduler s () -> Int -> m a -> m ()

-- | Batch is an artifical checkpoint that can be controlled by the user
--   throughout the lifetime of a scheduler.
data Batch s a

-- | Run a single batch of jobs. Supplied action will not return until all
--   jobs placed on the queue are done or the whole batch is cancelled with
--   one of these <a>cancelBatch</a>, <a>cancelBatch_</a> or
--   <a>cancelBatchWith</a>.
--   
--   It waits for all scheduled jobs to finish and collects the computed
--   results into a list. It is a blocking operation, but if there are no
--   jobs in progress it will return immediately. It is safe to continue
--   using the supplied scheduler after this function returns. However, if
--   any of the jobs resulted in an exception it will be rethrown by this
--   function, which, unless caught, will further put the scheduler in a
--   terminated state.
--   
--   It is important to note that any job that hasn't had its results
--   collected from the scheduler prior to starting the batch it will end
--   up on the batch result list.
runBatch :: MonadPrimBase s m => Scheduler s a -> (Batch s a -> m c) -> m [a]

-- | Same as <a>runBatch</a>, except it ignores results of computation
runBatch_ :: MonadPrimBase s m => Scheduler s () -> (Batch s () -> m c) -> m ()

-- | Same as <a>runBatch</a>, except it produces <a>Results</a> instead of
--   a list.
runBatchR :: MonadPrimBase s m => Scheduler s a -> (Batch s a -> m c) -> m (Results a)

-- | Cancel batch with supplied identifier, which will lead to scheduler to
--   return <a>FinishedEarly</a> result. This is an idempotent operation
--   and has no affect if currently running batch does not match supplied
--   identifier. Returns <a>False</a> when cancelling did not succeed due
--   to mismatched identifier or does not return at all since all jobs get
--   cancelled immediately. For trivial schedulers however there is no way
--   to perform concurrent cancelation and it will return <a>True</a>.
cancelBatch :: MonadPrim s m => Batch s a -> a -> m Bool

-- | Same as <a>cancelBatch</a>, but only works with schedulers that don't
--   care about results
cancelBatch_ :: MonadPrim s m => Batch s () -> m Bool

-- | Same as <a>cancelBatch_</a>, but the result of computation will be set
--   to <a>FinishedEarlyWith</a>
cancelBatchWith :: MonadPrim s m => Batch s a -> a -> m Bool

-- | Check if the supplied batch has already finished.
hasBatchFinished :: MonadPrim s m => Batch s a -> m Bool

-- | This function gives a way to get access to the main batch that started
--   implicitely.
getCurrentBatch :: MonadPrim s m => Scheduler s a -> m (Batch s a)

-- | As soon as possible try to terminate any computation that is being
--   performed by all workers managed by this scheduler and collect
--   whatever results have been computed, with supplied element guaranteed
--   to being the last one. In case when <a>Results</a> type is returned
--   this function will cause the scheduler to produce <a>FinishedEarly</a>
--   
--   <i>Important</i> - With <a>Seq</a> strategy this will not stop other
--   scheduled tasks from being computed, although it will make sure their
--   results are discarded.
terminate :: MonadPrim s m => Scheduler s a -> a -> m a

-- | Similar to <a>terminate</a>, but for a <a>Scheduler</a> that does not
--   keep any results of computation.
--   
--   <i>Important</i> - In case of <a>Seq</a> computation strategy this
--   function has no affect.
terminate_ :: MonadPrim s m => Scheduler s () -> m ()

-- | Same as <a>terminate</a>, but returning a single element list
--   containing the supplied argument. This can be very useful for parallel
--   search algorithms. In case when <a>Results</a> is the return type this
--   function will cause the scheduler to produce <a>FinishedEarlyWith</a>
--   
--   <i>Important</i> - Same as with <a>terminate</a>, when <a>Seq</a>
--   strategy is used, this will not prevent computation from continuing,
--   but the scheduler will return only the result supplied to this
--   function.
terminateWith :: MonadPrim s m => Scheduler s a -> a -> m a

-- | A unique id for the worker in the <a>Scheduler</a> context. It will
--   always be a number from <tt>0</tt> up to, but not including, the
--   number of workers a scheduler has, which in turn can always be
--   determined with <a>numWorkers</a> function.
newtype WorkerId
WorkerId :: Int -> WorkerId
[getWorkerId] :: WorkerId -> Int

-- | Each worker is capable of keeping it's own state, that can be share
--   for different schedulers, but not at the same time. In other words
--   using the same <a>WorkerStates</a> on <a>withSchedulerS</a>
--   concurrently will result in an error. Can be initialized with
--   <a>initWorkerStates</a>
data WorkerStates ws

-- | Get the number of workers. Will mainly depend on the computation
--   strategy and/or number of capabilities you have. Related function is
--   <a>getCompWorkers</a>.
numWorkers :: Scheduler s a -> Int

-- | Get the computation strategy the states where initialized with.
workerStatesComp :: WorkerStates ws -> Comp

-- | Initialize a separate state for each worker.
initWorkerStates :: MonadIO m => Comp -> (WorkerId -> m ws) -> m (WorkerStates ws)

-- | Computation strategy to use when scheduling work.
data Comp

-- | Sequential computation
Seq :: Comp

-- | Schedule workers to run on specific capabilities. Specifying an empty
--   list <tt><a>ParOn</a> []</tt> or using <a>Par</a> will result in
--   utilization of all available capabilities.
ParOn :: ![Int] -> Comp

-- | Specify the number of workers that will be handling all the jobs.
--   Difference from <a>ParOn</a> is that workers can jump between cores.
--   Using <tt><a>ParN</a> 0</tt> will result in using all available
--   capabilities.
ParN :: {-# UNPACK #-} !Word16 -> Comp

-- | Parallel computation using all available cores. Same as
--   <tt><a>ParOn</a> []</tt>
pattern Par :: Comp

-- | Parallel computation using all available cores. Same as
--   <tt><a>ParN</a> 0</tt>
pattern Par' :: Comp

-- | Figure out how many workers will this computation strategy create.
--   
--   <i>Note</i> - If at any point during program execution global number
--   of capabilities gets changed with <a>setNumCapabilities</a>, it will
--   have no affect on this function, unless it hasn't yet been called with
--   <a>Par</a> or <a>Par'</a> arguments.
getCompWorkers :: MonadIO m => Comp -> m Int

-- | Replicate an action <tt>n</tt> times and schedule them acccording to
--   the supplied computation strategy.
replicateConcurrently :: MonadUnliftIO m => Comp -> Int -> m a -> m [a]

-- | Just like <a>replicateConcurrently</a>, but discards the results of
--   computation.
replicateConcurrently_ :: MonadUnliftIO m => Comp -> Int -> m a -> m ()

-- | Map an action over each element of the <a>Traversable</a> <tt>t</tt>
--   acccording to the supplied computation strategy.
traverseConcurrently :: (MonadUnliftIO m, Traversable t) => Comp -> (a -> m b) -> t a -> m (t b)

-- | Just like <a>traverseConcurrently</a>, but restricted to
--   <a>Foldable</a> and discards the results of computation.
traverseConcurrently_ :: (MonadUnliftIO m, Foldable t) => Comp -> (a -> m b) -> t a -> m ()

-- | This is generally a faster way to traverse while ignoring the result
--   rather than using <a>mapM_</a>.
traverse_ :: (Applicative f, Foldable t) => (a -> f ()) -> t a -> f ()

-- | Exception that gets thrown whenever concurrent access is attempted to
--   the <a>WorkerStates</a>
data MutexException
MutexException :: MutexException


module Control.Scheduler.Global

-- | A thread safe wrapper around <a>Scheduler</a>, which allows it to be
--   reused indefinitely and globally if need be. There is one already
--   created in this library: <a>globalScheduler</a>
data GlobalScheduler

-- | Global scheduler with <a>Par</a> computation strategy that can be used
--   anytime using <a>withGlobalScheduler_</a>
globalScheduler :: GlobalScheduler

-- | Create a new global scheduler, in case a single one
--   <a>globalScheduler</a> is not sufficient. It is important to note that
--   too much parallelization can significantly degrate performance,
--   therefore it is best not to use more than one scheduler at a time.
newGlobalScheduler :: MonadIO m => Comp -> m GlobalScheduler

-- | Use the global scheduler if it is not busy, otherwise initialize a
--   temporary one. It means that this function by itself will not block,
--   but if the same global scheduler used concurrently other schedulers
--   might get created.
withGlobalScheduler_ :: MonadUnliftIO m => GlobalScheduler -> (Scheduler RealWorld () -> m a) -> m ()
